{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUw8GW5BzN6n",
        "outputId": "17917906-d8b1-4b42-8fed-a8aaecba3da7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Archive:  out.zip\n",
            "  inflating: out.csv                 \n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp /content/drive/MyDrive/out.zip .\n",
        "!unzip out.zip\n",
        "!rm out.zip\n",
        "!mv out.csv data.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "VxpnCWbnuvL2",
        "outputId": "e993ed78-3e75-4047-f95f-b2d0b4adb268"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   street  highCard   avgRank  flushPossible  boardPaired  hasFour  lc3flush  \\\n",
              "0       1        10  5.333333          False        False    False     False   \n",
              "1       1        10  5.333333          False        False    False     False   \n",
              "2       2        10  5.250000          False        False    False     False   \n",
              "3       2        10  5.250000          False        False    False     False   \n",
              "4       1        13  9.666667          False         True    False     False   \n",
              "\n",
              "   lc4flush  lc4straight  lcOvercard     plAgg     opAgg  numAggCS  isLastAgg  \\\n",
              "0     False        False       False  2.000000  0.333333         0       True   \n",
              "1     False        False       False  0.333333  1.000000         0      False   \n",
              "2     False        False       False  1.000000  0.250000         0       True   \n",
              "3     False        False       False  0.250000  1.500000         1      False   \n",
              "4     False        False       False  0.500000  2.000000         0      False   \n",
              "\n",
              "         spr  amtToCall  numPlayers  relPos result  \n",
              "0  11.111111   0.000000           2     0.5      C  \n",
              "1  11.111111   0.000000           2     1.0      C  \n",
              "2  11.111111   0.000000           2     0.5      R  \n",
              "3   7.074074   0.333333           2     1.0      F  \n",
              "4  15.076923   0.000000           2     0.5      C  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bd88dccf-f680-4935-bbf9-4974621c937c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>street</th>\n",
              "      <th>highCard</th>\n",
              "      <th>avgRank</th>\n",
              "      <th>flushPossible</th>\n",
              "      <th>boardPaired</th>\n",
              "      <th>hasFour</th>\n",
              "      <th>lc3flush</th>\n",
              "      <th>lc4flush</th>\n",
              "      <th>lc4straight</th>\n",
              "      <th>lcOvercard</th>\n",
              "      <th>plAgg</th>\n",
              "      <th>opAgg</th>\n",
              "      <th>numAggCS</th>\n",
              "      <th>isLastAgg</th>\n",
              "      <th>spr</th>\n",
              "      <th>amtToCall</th>\n",
              "      <th>numPlayers</th>\n",
              "      <th>relPos</th>\n",
              "      <th>result</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>5.333333</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>11.111111</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.5</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>5.333333</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>11.111111</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>5.250000</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>11.111111</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.5</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>5.250000</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>7.074074</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>F</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>9.666667</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>15.076923</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.5</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bd88dccf-f680-4935-bbf9-4974621c937c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bd88dccf-f680-4935-bbf9-4974621c937c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bd88dccf-f680-4935-bbf9-4974621c937c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('data.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tmz8OYxMzxK9"
      },
      "outputs": [],
      "source": [
        "def normMinMax(columns):\n",
        "    for col in columns:\n",
        "        minVal = df[col].min()\n",
        "        maxVal = df[col].max()\n",
        "        df[col] = df[col].apply(lambda x: (x - minVal) / (maxVal - minVal))\n",
        "\n",
        "def normUnbounded(columns):\n",
        "    for col in columns:\n",
        "        df[col] = df[col].apply(lambda x: x / (1 + x))\n",
        "\n",
        "def boolToInt(columns):\n",
        "    for col in columns:\n",
        "        df[col] = df[col].apply(lambda x: 1 if x == True else 0)\n",
        "\n",
        "def convertLabels(x):\n",
        "    if x == 'F':\n",
        "        return 0\n",
        "    elif x == 'C':\n",
        "        return 1\n",
        "    elif x == 'R':\n",
        "        return 2\n",
        "    else:\n",
        "        raise Exception('Uknown label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2FZ-0lqU5KMF"
      },
      "outputs": [],
      "source": [
        "cols = ['street', 'highCard', 'avgRank', 'numPlayers', 'relPos']\n",
        "normMinMax(cols)\n",
        "\n",
        "cols = ['plAgg', 'opAgg', 'numAggCS', 'spr']\n",
        "normUnbounded(cols)\n",
        "\n",
        "cols = ['flushPossible', 'boardPaired', 'hasFour', 'lc3flush', 'lc4flush', 'lc4straight', 'lcOvercard', 'isLastAgg']\n",
        "boolToInt(cols)\n",
        "\n",
        "df['result'] = df['result'].apply(convertLabels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "ckTAY3n-GOws",
        "outputId": "8479493c-8812-43a1-98bc-415d6ec47bda"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   street  highCard   avgRank  flushPossible  boardPaired  hasFour  lc3flush  \\\n",
              "0     0.0      0.75  0.361111              0            0        0         0   \n",
              "1     0.0      0.75  0.361111              0            0        0         0   \n",
              "2     0.5      0.75  0.354167              0            0        0         0   \n",
              "3     0.5      0.75  0.354167              0            0        0         0   \n",
              "4     0.0      1.00  0.722222              0            1        0         0   \n",
              "\n",
              "   lc4flush  lc4straight  lcOvercard     plAgg     opAgg  numAggCS  isLastAgg  \\\n",
              "0         0            0           0  0.666667  0.250000       0.0          1   \n",
              "1         0            0           0  0.250000  0.500000       0.0          0   \n",
              "2         0            0           0  0.500000  0.200000       0.0          1   \n",
              "3         0            0           0  0.200000  0.600000       0.5          0   \n",
              "4         0            0           0  0.333333  0.666667       0.0          0   \n",
              "\n",
              "        spr  amtToCall  numPlayers  relPos  result  \n",
              "0  0.917431   0.000000         0.0     0.4       1  \n",
              "1  0.917431   0.000000         0.0     1.0       1  \n",
              "2  0.917431   0.000000         0.0     0.4       2  \n",
              "3  0.876147   0.333333         0.0     1.0       0  \n",
              "4  0.937799   0.000000         0.0     0.4       1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0a23a720-746c-4537-a89f-edba50652750\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>street</th>\n",
              "      <th>highCard</th>\n",
              "      <th>avgRank</th>\n",
              "      <th>flushPossible</th>\n",
              "      <th>boardPaired</th>\n",
              "      <th>hasFour</th>\n",
              "      <th>lc3flush</th>\n",
              "      <th>lc4flush</th>\n",
              "      <th>lc4straight</th>\n",
              "      <th>lcOvercard</th>\n",
              "      <th>plAgg</th>\n",
              "      <th>opAgg</th>\n",
              "      <th>numAggCS</th>\n",
              "      <th>isLastAgg</th>\n",
              "      <th>spr</th>\n",
              "      <th>amtToCall</th>\n",
              "      <th>numPlayers</th>\n",
              "      <th>relPos</th>\n",
              "      <th>result</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.361111</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.917431</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.361111</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.917431</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.354167</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.917431</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.354167</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0.876147</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.722222</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.937799</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0a23a720-746c-4537-a89f-edba50652750')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0a23a720-746c-4537-a89f-edba50652750 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0a23a720-746c-4537-a89f-edba50652750');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAiRNo_JG1ms",
        "outputId": "5e6339bc-218c-490c-c585-4e5f4d520cf6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26199925"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jWGd0ZRETcz",
        "outputId": "698894cd-5b77-46dc-bfd5-2341c094aecc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    15078225\n",
              "2     6722445\n",
              "0     4399255\n",
              "Name: result, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "df['result'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "m6wVj_BXIoak"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "\n",
        "def splitData(df, seed=123):\n",
        "    y = df['result'].to_numpy(dtype=np.uint8, copy=True)\n",
        "    x = df.drop(['result'], axis=1).to_numpy(dtype=np.float32, copy=True)\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=seed)\n",
        "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=1/9, random_state=seed)\n",
        "    y_train = keras.utils.to_categorical(y_train, 3)\n",
        "    y_val = keras.utils.to_categorical(y_val, 3)\n",
        "    y_test = keras.utils.to_categorical(y_test, 3)\n",
        "    return x_train, x_val, x_test, y_train, y_val, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "XJ8iXuAAIrUd"
      },
      "outputs": [],
      "source": [
        "x_train, x_val, x_test, y_train, y_val, y_test = splitData(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "qgkPiCQThLDQ"
      },
      "outputs": [],
      "source": [
        "def createModel(steps=1, lr=0.001):\n",
        "    optimizer = keras.optimizers.RMSprop(learning_rate=lr)\n",
        "    model = keras.Sequential([\n",
        "        keras.Input(shape=(18,)),\n",
        "        keras.layers.Dense(16, activation='relu'),\n",
        "        keras.layers.Dense(12, activation='relu'),\n",
        "        keras.layers.Dense(8, activation='relu'),\n",
        "        keras.layers.Dense(6, activation='relu'),\n",
        "        keras.layers.Dense(3, activation='softmax')\n",
        "    ])\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer='rmsprop',\n",
        "        metrics=[keras.metrics.Precision(), keras.metrics.Recall()],\n",
        "        steps_per_execution=steps\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "w3K48lU5oXI4"
      },
      "outputs": [],
      "source": [
        "def trainModel(model, x_train, y_train, x_val, y_val, filepath='model.keras', batch_size=256, epochs=10):\n",
        "    callbacks = [\n",
        "        keras.callbacks.ModelCheckpoint(\n",
        "            filepath=filepath,\n",
        "            save_best_only=True,\n",
        "            monitor='val_loss'\n",
        "        )\n",
        "    ]\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=(x_val, y_val),\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "    return callbacks, history"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(filename, x_test, y_test, batch_size=2**14):\n",
        "    model = keras.models.load_model(filename)\n",
        "    loss, precision, recall = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "    print(f'Loss: {loss:.3f},', f'Precision: {precision:.3f},', f'Recall: {recall:.3f}')"
      ],
      "metadata": {
        "id": "w69KFUphUNPC"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimal_epoch(history):\n",
        "    val_loss = history.history['val_loss']\n",
        "    return val_loss.index(min(val_loss)) + 1"
      ],
      "metadata": {
        "id": "RK7bUGvfMmb2"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtjO9Df_hR5k",
        "outputId": "c98b0f70-cde5-447b-fa29-f5f7eb802eb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.78.25.50:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.78.25.50:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.78.25.50:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.78.25.50:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
        "strategy = tf.distribute.TPUStrategy(tpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfvWr58rDVH6",
        "outputId": "bbc73df5-c2de-47a1-beee-1242965e1c6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_29\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_68 (Dense)            (None, 16)                304       \n",
            "                                                                 \n",
            " dense_69 (Dense)            (None, 12)                204       \n",
            "                                                                 \n",
            " dense_70 (Dense)            (None, 8)                 104       \n",
            "                                                                 \n",
            " dense_71 (Dense)            (None, 6)                 54        \n",
            "                                                                 \n",
            " dense_72 (Dense)            (None, 3)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 687\n",
            "Trainable params: 687\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "with tf.distribute.TPUStrategy(tpu).scope():\n",
        "    model = createModel(steps=8, lr=0.005)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zC97_o0KEwqj",
        "outputId": "228fadb7-5ee7-45f9-aec2-02125e7e0ef4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1280/1280 [==============================] - 24s 19ms/step - loss: 0.7054 - precision_6: 0.6632 - recall_6: 0.5935 - val_loss: 0.6737 - val_precision_6: 0.6787 - val_recall_6: 0.6221\n",
            "Epoch 2/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6729 - precision_6: 0.6792 - recall_6: 0.6275 - val_loss: 0.6704 - val_precision_6: 0.6807 - val_recall_6: 0.6315\n",
            "Epoch 3/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6703 - precision_6: 0.6806 - recall_6: 0.6322 - val_loss: 0.6687 - val_precision_6: 0.6805 - val_recall_6: 0.6366\n",
            "Epoch 4/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6682 - precision_6: 0.6818 - recall_6: 0.6348 - val_loss: 0.6664 - val_precision_6: 0.6828 - val_recall_6: 0.6365\n",
            "Epoch 5/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6668 - precision_6: 0.6824 - recall_6: 0.6367 - val_loss: 0.6661 - val_precision_6: 0.6828 - val_recall_6: 0.6352\n",
            "Epoch 6/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6659 - precision_6: 0.6827 - recall_6: 0.6377 - val_loss: 0.6648 - val_precision_6: 0.6832 - val_recall_6: 0.6385\n",
            "Epoch 7/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6653 - precision_6: 0.6830 - recall_6: 0.6382 - val_loss: 0.6652 - val_precision_6: 0.6824 - val_recall_6: 0.6399\n",
            "Epoch 8/100\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6644 - precision_6: 0.6833 - recall_6: 0.6387 - val_loss: 0.6637 - val_precision_6: 0.6834 - val_recall_6: 0.6410\n",
            "Epoch 9/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6637 - precision_6: 0.6834 - recall_6: 0.6396 - val_loss: 0.6634 - val_precision_6: 0.6842 - val_recall_6: 0.6372\n",
            "Epoch 10/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6631 - precision_6: 0.6836 - recall_6: 0.6405 - val_loss: 0.6622 - val_precision_6: 0.6836 - val_recall_6: 0.6421\n",
            "Epoch 11/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6626 - precision_6: 0.6837 - recall_6: 0.6409 - val_loss: 0.6619 - val_precision_6: 0.6850 - val_recall_6: 0.6382\n",
            "Epoch 12/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6622 - precision_6: 0.6838 - recall_6: 0.6414 - val_loss: 0.6614 - val_precision_6: 0.6838 - val_recall_6: 0.6437\n",
            "Epoch 13/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6619 - precision_6: 0.6839 - recall_6: 0.6416 - val_loss: 0.6614 - val_precision_6: 0.6845 - val_recall_6: 0.6408\n",
            "Epoch 14/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6617 - precision_6: 0.6840 - recall_6: 0.6414 - val_loss: 0.6610 - val_precision_6: 0.6838 - val_recall_6: 0.6424\n",
            "Epoch 15/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6615 - precision_6: 0.6841 - recall_6: 0.6412 - val_loss: 0.6607 - val_precision_6: 0.6847 - val_recall_6: 0.6411\n",
            "Epoch 16/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6613 - precision_6: 0.6843 - recall_6: 0.6412 - val_loss: 0.6605 - val_precision_6: 0.6844 - val_recall_6: 0.6417\n",
            "Epoch 17/100\n",
            "1280/1280 [==============================] - 12s 9ms/step - loss: 0.6611 - precision_6: 0.6843 - recall_6: 0.6411 - val_loss: 0.6607 - val_precision_6: 0.6848 - val_recall_6: 0.6404\n",
            "Epoch 18/100\n",
            "1280/1280 [==============================] - 12s 9ms/step - loss: 0.6609 - precision_6: 0.6844 - recall_6: 0.6411 - val_loss: 0.6599 - val_precision_6: 0.6850 - val_recall_6: 0.6415\n",
            "Epoch 19/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6608 - precision_6: 0.6843 - recall_6: 0.6411 - val_loss: 0.6606 - val_precision_6: 0.6839 - val_recall_6: 0.6410\n",
            "Epoch 20/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6607 - precision_6: 0.6844 - recall_6: 0.6411 - val_loss: 0.6604 - val_precision_6: 0.6843 - val_recall_6: 0.6409\n",
            "Epoch 21/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6606 - precision_6: 0.6845 - recall_6: 0.6412 - val_loss: 0.6598 - val_precision_6: 0.6848 - val_recall_6: 0.6413\n",
            "Epoch 22/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6605 - precision_6: 0.6845 - recall_6: 0.6413 - val_loss: 0.6598 - val_precision_6: 0.6867 - val_recall_6: 0.6378\n",
            "Epoch 23/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6604 - precision_6: 0.6845 - recall_6: 0.6414 - val_loss: 0.6599 - val_precision_6: 0.6854 - val_recall_6: 0.6403\n",
            "Epoch 24/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6603 - precision_6: 0.6846 - recall_6: 0.6416 - val_loss: 0.6601 - val_precision_6: 0.6832 - val_recall_6: 0.6430\n",
            "Epoch 25/100\n",
            "1280/1280 [==============================] - 12s 9ms/step - loss: 0.6602 - precision_6: 0.6847 - recall_6: 0.6416 - val_loss: 0.6597 - val_precision_6: 0.6844 - val_recall_6: 0.6435\n",
            "Epoch 26/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6602 - precision_6: 0.6847 - recall_6: 0.6417 - val_loss: 0.6596 - val_precision_6: 0.6841 - val_recall_6: 0.6456\n",
            "Epoch 27/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6601 - precision_6: 0.6847 - recall_6: 0.6419 - val_loss: 0.6612 - val_precision_6: 0.6822 - val_recall_6: 0.6448\n",
            "Epoch 28/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6601 - precision_6: 0.6847 - recall_6: 0.6419 - val_loss: 0.6599 - val_precision_6: 0.6853 - val_recall_6: 0.6402\n",
            "Epoch 29/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6600 - precision_6: 0.6847 - recall_6: 0.6420 - val_loss: 0.6592 - val_precision_6: 0.6845 - val_recall_6: 0.6435\n",
            "Epoch 30/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6599 - precision_6: 0.6848 - recall_6: 0.6421 - val_loss: 0.6615 - val_precision_6: 0.6821 - val_recall_6: 0.6399\n",
            "Epoch 31/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6598 - precision_6: 0.6848 - recall_6: 0.6422 - val_loss: 0.6626 - val_precision_6: 0.6845 - val_recall_6: 0.6350\n",
            "Epoch 32/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6597 - precision_6: 0.6848 - recall_6: 0.6423 - val_loss: 0.6598 - val_precision_6: 0.6846 - val_recall_6: 0.6426\n",
            "Epoch 33/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6597 - precision_6: 0.6848 - recall_6: 0.6423 - val_loss: 0.6596 - val_precision_6: 0.6856 - val_recall_6: 0.6390\n",
            "Epoch 34/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6596 - precision_6: 0.6849 - recall_6: 0.6424 - val_loss: 0.6593 - val_precision_6: 0.6842 - val_recall_6: 0.6433\n",
            "Epoch 35/100\n",
            "1280/1280 [==============================] - 12s 9ms/step - loss: 0.6596 - precision_6: 0.6849 - recall_6: 0.6424 - val_loss: 0.6588 - val_precision_6: 0.6845 - val_recall_6: 0.6446\n",
            "Epoch 36/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6595 - precision_6: 0.6850 - recall_6: 0.6424 - val_loss: 0.6602 - val_precision_6: 0.6827 - val_recall_6: 0.6447\n",
            "Epoch 37/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6594 - precision_6: 0.6850 - recall_6: 0.6425 - val_loss: 0.6586 - val_precision_6: 0.6859 - val_recall_6: 0.6425\n",
            "Epoch 38/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6594 - precision_6: 0.6850 - recall_6: 0.6425 - val_loss: 0.6588 - val_precision_6: 0.6849 - val_recall_6: 0.6437\n",
            "Epoch 39/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6594 - precision_6: 0.6850 - recall_6: 0.6425 - val_loss: 0.6587 - val_precision_6: 0.6844 - val_recall_6: 0.6453\n",
            "Epoch 40/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6593 - precision_6: 0.6851 - recall_6: 0.6425 - val_loss: 0.6603 - val_precision_6: 0.6864 - val_recall_6: 0.6346\n",
            "Epoch 41/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6592 - precision_6: 0.6852 - recall_6: 0.6426 - val_loss: 0.6584 - val_precision_6: 0.6854 - val_recall_6: 0.6435\n",
            "Epoch 42/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6592 - precision_6: 0.6852 - recall_6: 0.6426 - val_loss: 0.6586 - val_precision_6: 0.6868 - val_recall_6: 0.6392\n",
            "Epoch 43/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6592 - precision_6: 0.6852 - recall_6: 0.6425 - val_loss: 0.6602 - val_precision_6: 0.6854 - val_recall_6: 0.6373\n",
            "Epoch 44/100\n",
            "1280/1280 [==============================] - 12s 9ms/step - loss: 0.6591 - precision_6: 0.6854 - recall_6: 0.6424 - val_loss: 0.6586 - val_precision_6: 0.6864 - val_recall_6: 0.6401\n",
            "Epoch 45/100\n",
            "1280/1280 [==============================] - 12s 9ms/step - loss: 0.6591 - precision_6: 0.6853 - recall_6: 0.6424 - val_loss: 0.6589 - val_precision_6: 0.6848 - val_recall_6: 0.6447\n",
            "Epoch 46/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6590 - precision_6: 0.6854 - recall_6: 0.6424 - val_loss: 0.6582 - val_precision_6: 0.6857 - val_recall_6: 0.6427\n",
            "Epoch 47/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6590 - precision_6: 0.6854 - recall_6: 0.6424 - val_loss: 0.6595 - val_precision_6: 0.6834 - val_recall_6: 0.6465\n",
            "Epoch 48/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6589 - precision_6: 0.6856 - recall_6: 0.6424 - val_loss: 0.6587 - val_precision_6: 0.6848 - val_recall_6: 0.6448\n",
            "Epoch 49/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6589 - precision_6: 0.6856 - recall_6: 0.6423 - val_loss: 0.6591 - val_precision_6: 0.6854 - val_recall_6: 0.6414\n",
            "Epoch 50/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6589 - precision_6: 0.6856 - recall_6: 0.6424 - val_loss: 0.6592 - val_precision_6: 0.6864 - val_recall_6: 0.6395\n",
            "Epoch 51/100\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6588 - precision_6: 0.6856 - recall_6: 0.6425 - val_loss: 0.6594 - val_precision_6: 0.6841 - val_recall_6: 0.6458\n",
            "Epoch 52/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6588 - precision_6: 0.6858 - recall_6: 0.6425 - val_loss: 0.6580 - val_precision_6: 0.6860 - val_recall_6: 0.6435\n",
            "Epoch 53/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6588 - precision_6: 0.6857 - recall_6: 0.6424 - val_loss: 0.6598 - val_precision_6: 0.6843 - val_recall_6: 0.6410\n",
            "Epoch 54/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6588 - precision_6: 0.6859 - recall_6: 0.6424 - val_loss: 0.6609 - val_precision_6: 0.6846 - val_recall_6: 0.6361\n",
            "Epoch 55/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6588 - precision_6: 0.6859 - recall_6: 0.6423 - val_loss: 0.6579 - val_precision_6: 0.6856 - val_recall_6: 0.6449\n",
            "Epoch 56/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6587 - precision_6: 0.6859 - recall_6: 0.6424 - val_loss: 0.6584 - val_precision_6: 0.6867 - val_recall_6: 0.6399\n",
            "Epoch 57/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6587 - precision_6: 0.6859 - recall_6: 0.6424 - val_loss: 0.6582 - val_precision_6: 0.6869 - val_recall_6: 0.6405\n",
            "Epoch 58/100\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6587 - precision_6: 0.6860 - recall_6: 0.6424 - val_loss: 0.6581 - val_precision_6: 0.6858 - val_recall_6: 0.6439\n",
            "Epoch 59/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6586 - precision_6: 0.6860 - recall_6: 0.6424 - val_loss: 0.6578 - val_precision_6: 0.6858 - val_recall_6: 0.6448\n",
            "Epoch 60/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6586 - precision_6: 0.6861 - recall_6: 0.6424 - val_loss: 0.6581 - val_precision_6: 0.6871 - val_recall_6: 0.6406\n",
            "Epoch 61/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6587 - precision_6: 0.6860 - recall_6: 0.6424 - val_loss: 0.6583 - val_precision_6: 0.6875 - val_recall_6: 0.6396\n",
            "Epoch 62/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6586 - precision_6: 0.6861 - recall_6: 0.6424 - val_loss: 0.6584 - val_precision_6: 0.6853 - val_recall_6: 0.6457\n",
            "Epoch 63/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6586 - precision_6: 0.6861 - recall_6: 0.6424 - val_loss: 0.6584 - val_precision_6: 0.6895 - val_recall_6: 0.6354\n",
            "Epoch 64/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6586 - precision_6: 0.6861 - recall_6: 0.6424 - val_loss: 0.6580 - val_precision_6: 0.6874 - val_recall_6: 0.6410\n",
            "Epoch 65/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6585 - precision_6: 0.6862 - recall_6: 0.6424 - val_loss: 0.6586 - val_precision_6: 0.6847 - val_recall_6: 0.6454\n",
            "Epoch 66/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6585 - precision_6: 0.6862 - recall_6: 0.6424 - val_loss: 0.6576 - val_precision_6: 0.6868 - val_recall_6: 0.6430\n",
            "Epoch 67/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6584 - precision_6: 0.6863 - recall_6: 0.6425 - val_loss: 0.6590 - val_precision_6: 0.6847 - val_recall_6: 0.6419\n",
            "Epoch 68/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6584 - precision_6: 0.6862 - recall_6: 0.6425 - val_loss: 0.6582 - val_precision_6: 0.6875 - val_recall_6: 0.6390\n",
            "Epoch 69/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6584 - precision_6: 0.6862 - recall_6: 0.6424 - val_loss: 0.6592 - val_precision_6: 0.6844 - val_recall_6: 0.6450\n",
            "Epoch 70/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6584 - precision_6: 0.6863 - recall_6: 0.6425 - val_loss: 0.6576 - val_precision_6: 0.6872 - val_recall_6: 0.6418\n",
            "Epoch 71/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6584 - precision_6: 0.6862 - recall_6: 0.6426 - val_loss: 0.6584 - val_precision_6: 0.6872 - val_recall_6: 0.6400\n",
            "Epoch 72/100\n",
            "1280/1280 [==============================] - 13s 10ms/step - loss: 0.6583 - precision_6: 0.6863 - recall_6: 0.6426 - val_loss: 0.6580 - val_precision_6: 0.6863 - val_recall_6: 0.6420\n",
            "Epoch 73/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6583 - precision_6: 0.6863 - recall_6: 0.6426 - val_loss: 0.6580 - val_precision_6: 0.6877 - val_recall_6: 0.6398\n",
            "Epoch 74/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6583 - precision_6: 0.6863 - recall_6: 0.6427 - val_loss: 0.6577 - val_precision_6: 0.6856 - val_recall_6: 0.6454\n",
            "Epoch 75/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6583 - precision_6: 0.6863 - recall_6: 0.6427 - val_loss: 0.6584 - val_precision_6: 0.6848 - val_recall_6: 0.6445\n",
            "Epoch 76/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6583 - precision_6: 0.6862 - recall_6: 0.6427 - val_loss: 0.6589 - val_precision_6: 0.6842 - val_recall_6: 0.6457\n",
            "Epoch 77/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6583 - precision_6: 0.6863 - recall_6: 0.6427 - val_loss: 0.6581 - val_precision_6: 0.6858 - val_recall_6: 0.6439\n",
            "Epoch 78/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6583 - precision_6: 0.6863 - recall_6: 0.6428 - val_loss: 0.6589 - val_precision_6: 0.6883 - val_recall_6: 0.6359\n",
            "Epoch 79/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6583 - precision_6: 0.6863 - recall_6: 0.6427 - val_loss: 0.6577 - val_precision_6: 0.6859 - val_recall_6: 0.6450\n",
            "Epoch 80/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6582 - precision_6: 0.6863 - recall_6: 0.6429 - val_loss: 0.6579 - val_precision_6: 0.6852 - val_recall_6: 0.6450\n",
            "Epoch 81/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6582 - precision_6: 0.6863 - recall_6: 0.6428 - val_loss: 0.6582 - val_precision_6: 0.6863 - val_recall_6: 0.6426\n",
            "Epoch 82/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6582 - precision_6: 0.6864 - recall_6: 0.6429 - val_loss: 0.6586 - val_precision_6: 0.6845 - val_recall_6: 0.6469\n",
            "Epoch 83/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6583 - precision_6: 0.6863 - recall_6: 0.6428 - val_loss: 0.6579 - val_precision_6: 0.6870 - val_recall_6: 0.6412\n",
            "Epoch 84/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6583 - precision_6: 0.6862 - recall_6: 0.6427 - val_loss: 0.6574 - val_precision_6: 0.6863 - val_recall_6: 0.6445\n",
            "Epoch 85/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6582 - precision_6: 0.6864 - recall_6: 0.6429 - val_loss: 0.6591 - val_precision_6: 0.6841 - val_recall_6: 0.6446\n",
            "Epoch 86/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6582 - precision_6: 0.6863 - recall_6: 0.6429 - val_loss: 0.6584 - val_precision_6: 0.6844 - val_recall_6: 0.6470\n",
            "Epoch 87/100\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6582 - precision_6: 0.6863 - recall_6: 0.6429 - val_loss: 0.6580 - val_precision_6: 0.6854 - val_recall_6: 0.6452\n",
            "Epoch 88/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6582 - precision_6: 0.6863 - recall_6: 0.6428 - val_loss: 0.6591 - val_precision_6: 0.6869 - val_recall_6: 0.6381\n",
            "Epoch 89/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6582 - precision_6: 0.6863 - recall_6: 0.6429 - val_loss: 0.6576 - val_precision_6: 0.6872 - val_recall_6: 0.6417\n",
            "Epoch 90/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6582 - precision_6: 0.6863 - recall_6: 0.6428 - val_loss: 0.6585 - val_precision_6: 0.6844 - val_recall_6: 0.6445\n",
            "Epoch 91/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6582 - precision_6: 0.6863 - recall_6: 0.6428 - val_loss: 0.6581 - val_precision_6: 0.6870 - val_recall_6: 0.6402\n",
            "Epoch 92/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6581 - precision_6: 0.6864 - recall_6: 0.6429 - val_loss: 0.6585 - val_precision_6: 0.6887 - val_recall_6: 0.6378\n",
            "Epoch 93/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6581 - precision_6: 0.6864 - recall_6: 0.6429 - val_loss: 0.6575 - val_precision_6: 0.6867 - val_recall_6: 0.6434\n",
            "Epoch 94/100\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6581 - precision_6: 0.6864 - recall_6: 0.6429 - val_loss: 0.6593 - val_precision_6: 0.6874 - val_recall_6: 0.6383\n",
            "Epoch 95/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6581 - precision_6: 0.6865 - recall_6: 0.6429 - val_loss: 0.6590 - val_precision_6: 0.6872 - val_recall_6: 0.6376\n",
            "Epoch 96/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6581 - precision_6: 0.6864 - recall_6: 0.6430 - val_loss: 0.6580 - val_precision_6: 0.6862 - val_recall_6: 0.6427\n",
            "Epoch 97/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6581 - precision_6: 0.6864 - recall_6: 0.6431 - val_loss: 0.6576 - val_precision_6: 0.6862 - val_recall_6: 0.6441\n",
            "Epoch 98/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6581 - precision_6: 0.6864 - recall_6: 0.6430 - val_loss: 0.6574 - val_precision_6: 0.6873 - val_recall_6: 0.6418\n",
            "Epoch 99/100\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6581 - precision_6: 0.6864 - recall_6: 0.6431 - val_loss: 0.6574 - val_precision_6: 0.6855 - val_recall_6: 0.6461\n",
            "Epoch 100/100\n",
            "1280/1280 [==============================] - 12s 9ms/step - loss: 0.6581 - precision_6: 0.6864 - recall_6: 0.6431 - val_loss: 0.6585 - val_precision_6: 0.6878 - val_recall_6: 0.6375\n"
          ]
        }
      ],
      "source": [
        "callbacks, history = trainModel(model, x_train, y_train, x_val, y_val, filepath='model-tpu.keras', batch_size=2**14, epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVqS6cDZJ5HQ",
        "outputId": "b32c80d5-90ff-4993-eb72-95d55421eb3d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "84"
            ]
          },
          "execution_count": 138,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_optimal_epoch(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lr1DV8lVXMwg",
        "outputId": "f90f0938-8721-4395-bb9f-8820c8693c60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_12 (Dense)            (None, 3)                 57        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 57\n",
            "Trainable params: 57\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "with strategy.scope():\n",
        "    model = createModel(steps=8, lr=0.005)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ufv_3YxYYbPT",
        "outputId": "cd1543e0-b65c-4fa5-c5e8-5d53cb42ea02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1280/1280 [==============================] - 15s 11ms/step - loss: 0.6875 - precision_5: 0.6616 - recall_5: 0.6166 - val_loss: 0.6873 - val_precision_5: 0.6591 - val_recall_5: 0.6164\n",
            "Epoch 2/30\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6875 - precision_5: 0.6615 - recall_5: 0.6166 - val_loss: 0.6873 - val_precision_5: 0.6600 - val_recall_5: 0.6168\n",
            "Epoch 3/30\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6875 - precision_5: 0.6616 - recall_5: 0.6166 - val_loss: 0.6874 - val_precision_5: 0.6658 - val_recall_5: 0.6188\n",
            "Epoch 4/30\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6875 - precision_5: 0.6616 - recall_5: 0.6168 - val_loss: 0.6874 - val_precision_5: 0.6656 - val_recall_5: 0.6190\n",
            "Epoch 5/30\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6875 - precision_5: 0.6615 - recall_5: 0.6168 - val_loss: 0.6873 - val_precision_5: 0.6617 - val_recall_5: 0.6171\n",
            "Epoch 6/30\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6875 - precision_5: 0.6616 - recall_5: 0.6167 - val_loss: 0.6873 - val_precision_5: 0.6613 - val_recall_5: 0.6163\n",
            "Epoch 7/30\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6875 - precision_5: 0.6616 - recall_5: 0.6168 - val_loss: 0.6873 - val_precision_5: 0.6605 - val_recall_5: 0.6168\n",
            "Epoch 8/30\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6875 - precision_5: 0.6616 - recall_5: 0.6168 - val_loss: 0.6873 - val_precision_5: 0.6631 - val_recall_5: 0.6162\n",
            "Epoch 9/30\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6875 - precision_5: 0.6616 - recall_5: 0.6169 - val_loss: 0.6873 - val_precision_5: 0.6649 - val_recall_5: 0.6169\n",
            "Epoch 10/30\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6875 - precision_5: 0.6616 - recall_5: 0.6168 - val_loss: 0.6873 - val_precision_5: 0.6649 - val_recall_5: 0.6183\n",
            "Epoch 11/30\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6875 - precision_5: 0.6616 - recall_5: 0.6169 - val_loss: 0.6873 - val_precision_5: 0.6604 - val_recall_5: 0.6170\n",
            "Epoch 12/30\n",
            "1280/1280 [==============================] - 12s 9ms/step - loss: 0.6875 - precision_5: 0.6616 - recall_5: 0.6169 - val_loss: 0.6873 - val_precision_5: 0.6626 - val_recall_5: 0.6162\n",
            "Epoch 13/30\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6875 - precision_5: 0.6616 - recall_5: 0.6169 - val_loss: 0.6873 - val_precision_5: 0.6586 - val_recall_5: 0.6173\n",
            "Epoch 14/30\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6875 - precision_5: 0.6615 - recall_5: 0.6168 - val_loss: 0.6873 - val_precision_5: 0.6620 - val_recall_5: 0.6166\n",
            "Epoch 15/30\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6875 - precision_5: 0.6616 - recall_5: 0.6169 - val_loss: 0.6873 - val_precision_5: 0.6634 - val_recall_5: 0.6170\n",
            "Epoch 16/30\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6875 - precision_5: 0.6616 - recall_5: 0.6169 - val_loss: 0.6873 - val_precision_5: 0.6591 - val_recall_5: 0.6165\n",
            "Epoch 17/30\n",
            "1280/1280 [==============================] - 12s 9ms/step - loss: 0.6875 - precision_5: 0.6615 - recall_5: 0.6168 - val_loss: 0.6873 - val_precision_5: 0.6596 - val_recall_5: 0.6170\n",
            "Epoch 18/30\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6875 - precision_5: 0.6616 - recall_5: 0.6169 - val_loss: 0.6873 - val_precision_5: 0.6596 - val_recall_5: 0.6160\n",
            "Epoch 19/30\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6875 - precision_5: 0.6615 - recall_5: 0.6169 - val_loss: 0.6874 - val_precision_5: 0.6653 - val_recall_5: 0.6179\n",
            "Epoch 20/30\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6875 - precision_5: 0.6616 - recall_5: 0.6169 - val_loss: 0.6873 - val_precision_5: 0.6596 - val_recall_5: 0.6172\n",
            "Epoch 21/30\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6875 - precision_5: 0.6616 - recall_5: 0.6169 - val_loss: 0.6873 - val_precision_5: 0.6613 - val_recall_5: 0.6165\n",
            "Epoch 22/30\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6875 - precision_5: 0.6616 - recall_5: 0.6170 - val_loss: 0.6873 - val_precision_5: 0.6609 - val_recall_5: 0.6165\n",
            "Epoch 23/30\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6875 - precision_5: 0.6616 - recall_5: 0.6169 - val_loss: 0.6875 - val_precision_5: 0.6663 - val_recall_5: 0.6196\n",
            "Epoch 24/30\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6875 - precision_5: 0.6616 - recall_5: 0.6169 - val_loss: 0.6873 - val_precision_5: 0.6624 - val_recall_5: 0.6166\n",
            "Epoch 25/30\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6875 - precision_5: 0.6616 - recall_5: 0.6169 - val_loss: 0.6873 - val_precision_5: 0.6632 - val_recall_5: 0.6171\n",
            "Epoch 26/30\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6875 - precision_5: 0.6616 - recall_5: 0.6169 - val_loss: 0.6873 - val_precision_5: 0.6588 - val_recall_5: 0.6168\n",
            "Epoch 27/30\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6875 - precision_5: 0.6616 - recall_5: 0.6169 - val_loss: 0.6873 - val_precision_5: 0.6620 - val_recall_5: 0.6165\n",
            "Epoch 28/30\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6875 - precision_5: 0.6616 - recall_5: 0.6169 - val_loss: 0.6873 - val_precision_5: 0.6591 - val_recall_5: 0.6169\n",
            "Epoch 29/30\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6875 - precision_5: 0.6615 - recall_5: 0.6168 - val_loss: 0.6873 - val_precision_5: 0.6612 - val_recall_5: 0.6170\n",
            "Epoch 30/30\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6875 - precision_5: 0.6616 - recall_5: 0.6169 - val_loss: 0.6873 - val_precision_5: 0.6646 - val_recall_5: 0.6175\n"
          ]
        }
      ],
      "source": [
        "callbacks, history = trainModel(model, x_train, y_train, x_val, y_val, filepath='linear.keras', batch_size=2**14, epochs=30)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model('linear.keras', x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "We2BdTQNNVLZ",
        "outputId": "800f701f-d30d-4711-fb46-ce8a1a115ede"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "160/160 [==============================] - 2s 11ms/step - loss: 0.6878 - precision_5: 0.6604 - recall_5: 0.6166\n",
            "Loss: 0.688, Precision: 0.660, Recall: 0.617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "    model = createModel(steps=8, lr=0.005)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0a2cQnuQxWM",
        "outputId": "fa31c9e3-3909-4dab-e6b8-99a8d6282c7b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_13 (Dense)            (None, 16)                304       \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 12)                204       \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 8)                 104       \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 6)                 54        \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 3)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 687\n",
            "Trainable params: 687\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks, history = trainModel(model, x_train, y_train, x_val, y_val, filepath='model-tpu.keras', batch_size=2**14, epochs=300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RjaYHU7XH-q",
        "outputId": "707ba455-7c7f-4989-e6b5-bf4cf0d13629"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "1280/1280 [==============================] - 24s 18ms/step - loss: 0.7108 - precision_6: 0.6640 - recall_6: 0.5910 - val_loss: 0.6720 - val_precision_6: 0.6782 - val_recall_6: 0.6306\n",
            "Epoch 2/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6710 - precision_6: 0.6794 - recall_6: 0.6305 - val_loss: 0.6698 - val_precision_6: 0.6803 - val_recall_6: 0.6285\n",
            "Epoch 3/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6679 - precision_6: 0.6814 - recall_6: 0.6325 - val_loss: 0.6657 - val_precision_6: 0.6834 - val_recall_6: 0.6339\n",
            "Epoch 4/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6661 - precision_6: 0.6819 - recall_6: 0.6349 - val_loss: 0.6645 - val_precision_6: 0.6824 - val_recall_6: 0.6381\n",
            "Epoch 5/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6651 - precision_6: 0.6824 - recall_6: 0.6368 - val_loss: 0.6637 - val_precision_6: 0.6829 - val_recall_6: 0.6384\n",
            "Epoch 6/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6643 - precision_6: 0.6825 - recall_6: 0.6383 - val_loss: 0.6643 - val_precision_6: 0.6808 - val_recall_6: 0.6416\n",
            "Epoch 7/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6637 - precision_6: 0.6827 - recall_6: 0.6393 - val_loss: 0.6625 - val_precision_6: 0.6846 - val_recall_6: 0.6380\n",
            "Epoch 8/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6631 - precision_6: 0.6829 - recall_6: 0.6401 - val_loss: 0.6627 - val_precision_6: 0.6836 - val_recall_6: 0.6390\n",
            "Epoch 9/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6627 - precision_6: 0.6832 - recall_6: 0.6404 - val_loss: 0.6642 - val_precision_6: 0.6805 - val_recall_6: 0.6416\n",
            "Epoch 10/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6624 - precision_6: 0.6834 - recall_6: 0.6407 - val_loss: 0.6614 - val_precision_6: 0.6836 - val_recall_6: 0.6420\n",
            "Epoch 11/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6622 - precision_6: 0.6835 - recall_6: 0.6409 - val_loss: 0.6622 - val_precision_6: 0.6822 - val_recall_6: 0.6419\n",
            "Epoch 12/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6619 - precision_6: 0.6835 - recall_6: 0.6411 - val_loss: 0.6617 - val_precision_6: 0.6828 - val_recall_6: 0.6444\n",
            "Epoch 13/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6617 - precision_6: 0.6836 - recall_6: 0.6413 - val_loss: 0.6631 - val_precision_6: 0.6810 - val_recall_6: 0.6438\n",
            "Epoch 14/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6615 - precision_6: 0.6837 - recall_6: 0.6414 - val_loss: 0.6623 - val_precision_6: 0.6804 - val_recall_6: 0.6453\n",
            "Epoch 15/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6613 - precision_6: 0.6837 - recall_6: 0.6414 - val_loss: 0.6603 - val_precision_6: 0.6837 - val_recall_6: 0.6430\n",
            "Epoch 16/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6612 - precision_6: 0.6838 - recall_6: 0.6415 - val_loss: 0.6616 - val_precision_6: 0.6844 - val_recall_6: 0.6383\n",
            "Epoch 17/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6610 - precision_6: 0.6839 - recall_6: 0.6415 - val_loss: 0.6614 - val_precision_6: 0.6833 - val_recall_6: 0.6406\n",
            "Epoch 18/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6608 - precision_6: 0.6839 - recall_6: 0.6416 - val_loss: 0.6606 - val_precision_6: 0.6834 - val_recall_6: 0.6419\n",
            "Epoch 19/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6607 - precision_6: 0.6840 - recall_6: 0.6417 - val_loss: 0.6601 - val_precision_6: 0.6833 - val_recall_6: 0.6447\n",
            "Epoch 20/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6606 - precision_6: 0.6840 - recall_6: 0.6419 - val_loss: 0.6596 - val_precision_6: 0.6850 - val_recall_6: 0.6412\n",
            "Epoch 21/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6605 - precision_6: 0.6840 - recall_6: 0.6420 - val_loss: 0.6603 - val_precision_6: 0.6848 - val_recall_6: 0.6390\n",
            "Epoch 22/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6604 - precision_6: 0.6840 - recall_6: 0.6421 - val_loss: 0.6604 - val_precision_6: 0.6850 - val_recall_6: 0.6380\n",
            "Epoch 23/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6602 - precision_6: 0.6842 - recall_6: 0.6423 - val_loss: 0.6591 - val_precision_6: 0.6850 - val_recall_6: 0.6421\n",
            "Epoch 24/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6601 - precision_6: 0.6843 - recall_6: 0.6424 - val_loss: 0.6613 - val_precision_6: 0.6848 - val_recall_6: 0.6381\n",
            "Epoch 25/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6600 - precision_6: 0.6843 - recall_6: 0.6425 - val_loss: 0.6591 - val_precision_6: 0.6847 - val_recall_6: 0.6427\n",
            "Epoch 26/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6599 - precision_6: 0.6844 - recall_6: 0.6426 - val_loss: 0.6591 - val_precision_6: 0.6851 - val_recall_6: 0.6414\n",
            "Epoch 27/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6597 - precision_6: 0.6844 - recall_6: 0.6428 - val_loss: 0.6588 - val_precision_6: 0.6844 - val_recall_6: 0.6446\n",
            "Epoch 28/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6597 - precision_6: 0.6844 - recall_6: 0.6428 - val_loss: 0.6595 - val_precision_6: 0.6850 - val_recall_6: 0.6400\n",
            "Epoch 29/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6596 - precision_6: 0.6845 - recall_6: 0.6429 - val_loss: 0.6600 - val_precision_6: 0.6830 - val_recall_6: 0.6480\n",
            "Epoch 30/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6596 - precision_6: 0.6845 - recall_6: 0.6430 - val_loss: 0.6585 - val_precision_6: 0.6854 - val_recall_6: 0.6430\n",
            "Epoch 31/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6594 - precision_6: 0.6845 - recall_6: 0.6431 - val_loss: 0.6589 - val_precision_6: 0.6843 - val_recall_6: 0.6431\n",
            "Epoch 32/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6594 - precision_6: 0.6845 - recall_6: 0.6430 - val_loss: 0.6589 - val_precision_6: 0.6841 - val_recall_6: 0.6448\n",
            "Epoch 33/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6593 - precision_6: 0.6846 - recall_6: 0.6431 - val_loss: 0.6588 - val_precision_6: 0.6860 - val_recall_6: 0.6399\n",
            "Epoch 34/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6593 - precision_6: 0.6846 - recall_6: 0.6431 - val_loss: 0.6594 - val_precision_6: 0.6856 - val_recall_6: 0.6397\n",
            "Epoch 35/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6592 - precision_6: 0.6845 - recall_6: 0.6433 - val_loss: 0.6588 - val_precision_6: 0.6843 - val_recall_6: 0.6461\n",
            "Epoch 36/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6592 - precision_6: 0.6846 - recall_6: 0.6433 - val_loss: 0.6596 - val_precision_6: 0.6821 - val_recall_6: 0.6451\n",
            "Epoch 37/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6592 - precision_6: 0.6846 - recall_6: 0.6433 - val_loss: 0.6588 - val_precision_6: 0.6851 - val_recall_6: 0.6419\n",
            "Epoch 38/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6591 - precision_6: 0.6846 - recall_6: 0.6434 - val_loss: 0.6593 - val_precision_6: 0.6834 - val_recall_6: 0.6467\n",
            "Epoch 39/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6591 - precision_6: 0.6846 - recall_6: 0.6434 - val_loss: 0.6583 - val_precision_6: 0.6840 - val_recall_6: 0.6475\n",
            "Epoch 40/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6591 - precision_6: 0.6847 - recall_6: 0.6434 - val_loss: 0.6585 - val_precision_6: 0.6838 - val_recall_6: 0.6454\n",
            "Epoch 41/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6590 - precision_6: 0.6847 - recall_6: 0.6435 - val_loss: 0.6587 - val_precision_6: 0.6837 - val_recall_6: 0.6461\n",
            "Epoch 42/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6589 - precision_6: 0.6847 - recall_6: 0.6435 - val_loss: 0.6609 - val_precision_6: 0.6814 - val_recall_6: 0.6489\n",
            "Epoch 43/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6589 - precision_6: 0.6848 - recall_6: 0.6434 - val_loss: 0.6595 - val_precision_6: 0.6828 - val_recall_6: 0.6489\n",
            "Epoch 44/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6589 - precision_6: 0.6848 - recall_6: 0.6434 - val_loss: 0.6583 - val_precision_6: 0.6869 - val_recall_6: 0.6398\n",
            "Epoch 45/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6589 - precision_6: 0.6848 - recall_6: 0.6434 - val_loss: 0.6583 - val_precision_6: 0.6856 - val_recall_6: 0.6419\n",
            "Epoch 46/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6589 - precision_6: 0.6848 - recall_6: 0.6433 - val_loss: 0.6584 - val_precision_6: 0.6863 - val_recall_6: 0.6407\n",
            "Epoch 47/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6589 - precision_6: 0.6848 - recall_6: 0.6434 - val_loss: 0.6602 - val_precision_6: 0.6830 - val_recall_6: 0.6455\n",
            "Epoch 48/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6589 - precision_6: 0.6848 - recall_6: 0.6432 - val_loss: 0.6590 - val_precision_6: 0.6840 - val_recall_6: 0.6428\n",
            "Epoch 49/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6588 - precision_6: 0.6849 - recall_6: 0.6433 - val_loss: 0.6583 - val_precision_6: 0.6859 - val_recall_6: 0.6412\n",
            "Epoch 50/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6588 - precision_6: 0.6849 - recall_6: 0.6432 - val_loss: 0.6593 - val_precision_6: 0.6838 - val_recall_6: 0.6412\n",
            "Epoch 51/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6588 - precision_6: 0.6849 - recall_6: 0.6433 - val_loss: 0.6597 - val_precision_6: 0.6818 - val_recall_6: 0.6438\n",
            "Epoch 52/300\n",
            "1280/1280 [==============================] - 12s 9ms/step - loss: 0.6587 - precision_6: 0.6849 - recall_6: 0.6433 - val_loss: 0.6587 - val_precision_6: 0.6836 - val_recall_6: 0.6488\n",
            "Epoch 53/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6587 - precision_6: 0.6849 - recall_6: 0.6433 - val_loss: 0.6598 - val_precision_6: 0.6814 - val_recall_6: 0.6449\n",
            "Epoch 54/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6587 - precision_6: 0.6850 - recall_6: 0.6433 - val_loss: 0.6587 - val_precision_6: 0.6835 - val_recall_6: 0.6486\n",
            "Epoch 55/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6587 - precision_6: 0.6849 - recall_6: 0.6433 - val_loss: 0.6591 - val_precision_6: 0.6849 - val_recall_6: 0.6426\n",
            "Epoch 56/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6586 - precision_6: 0.6850 - recall_6: 0.6433 - val_loss: 0.6594 - val_precision_6: 0.6858 - val_recall_6: 0.6384\n",
            "Epoch 57/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6586 - precision_6: 0.6850 - recall_6: 0.6433 - val_loss: 0.6580 - val_precision_6: 0.6860 - val_recall_6: 0.6433\n",
            "Epoch 58/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6586 - precision_6: 0.6851 - recall_6: 0.6434 - val_loss: 0.6578 - val_precision_6: 0.6854 - val_recall_6: 0.6439\n",
            "Epoch 59/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6585 - precision_6: 0.6851 - recall_6: 0.6434 - val_loss: 0.6581 - val_precision_6: 0.6848 - val_recall_6: 0.6438\n",
            "Epoch 60/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6585 - precision_6: 0.6851 - recall_6: 0.6433 - val_loss: 0.6578 - val_precision_6: 0.6863 - val_recall_6: 0.6422\n",
            "Epoch 61/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6585 - precision_6: 0.6850 - recall_6: 0.6434 - val_loss: 0.6591 - val_precision_6: 0.6841 - val_recall_6: 0.6448\n",
            "Epoch 62/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6585 - precision_6: 0.6850 - recall_6: 0.6434 - val_loss: 0.6598 - val_precision_6: 0.6823 - val_recall_6: 0.6459\n",
            "Epoch 63/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6584 - precision_6: 0.6851 - recall_6: 0.6434 - val_loss: 0.6592 - val_precision_6: 0.6870 - val_recall_6: 0.6366\n",
            "Epoch 64/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6584 - precision_6: 0.6851 - recall_6: 0.6434 - val_loss: 0.6582 - val_precision_6: 0.6838 - val_recall_6: 0.6454\n",
            "Epoch 65/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6584 - precision_6: 0.6851 - recall_6: 0.6434 - val_loss: 0.6585 - val_precision_6: 0.6860 - val_recall_6: 0.6398\n",
            "Epoch 66/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6584 - precision_6: 0.6852 - recall_6: 0.6434 - val_loss: 0.6584 - val_precision_6: 0.6854 - val_recall_6: 0.6416\n",
            "Epoch 67/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6583 - precision_6: 0.6852 - recall_6: 0.6434 - val_loss: 0.6583 - val_precision_6: 0.6865 - val_recall_6: 0.6394\n",
            "Epoch 68/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6583 - precision_6: 0.6852 - recall_6: 0.6434 - val_loss: 0.6583 - val_precision_6: 0.6860 - val_recall_6: 0.6406\n",
            "Epoch 69/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6584 - precision_6: 0.6852 - recall_6: 0.6433 - val_loss: 0.6579 - val_precision_6: 0.6848 - val_recall_6: 0.6463\n",
            "Epoch 70/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6583 - precision_6: 0.6852 - recall_6: 0.6434 - val_loss: 0.6590 - val_precision_6: 0.6827 - val_recall_6: 0.6440\n",
            "Epoch 71/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6583 - precision_6: 0.6852 - recall_6: 0.6434 - val_loss: 0.6592 - val_precision_6: 0.6867 - val_recall_6: 0.6361\n",
            "Epoch 72/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6583 - precision_6: 0.6852 - recall_6: 0.6435 - val_loss: 0.6579 - val_precision_6: 0.6854 - val_recall_6: 0.6442\n",
            "Epoch 73/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6583 - precision_6: 0.6852 - recall_6: 0.6434 - val_loss: 0.6575 - val_precision_6: 0.6857 - val_recall_6: 0.6438\n",
            "Epoch 74/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6583 - precision_6: 0.6851 - recall_6: 0.6434 - val_loss: 0.6582 - val_precision_6: 0.6847 - val_recall_6: 0.6435\n",
            "Epoch 75/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6583 - precision_6: 0.6852 - recall_6: 0.6434 - val_loss: 0.6578 - val_precision_6: 0.6844 - val_recall_6: 0.6474\n",
            "Epoch 76/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6582 - precision_6: 0.6852 - recall_6: 0.6435 - val_loss: 0.6594 - val_precision_6: 0.6830 - val_recall_6: 0.6432\n",
            "Epoch 77/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6582 - precision_6: 0.6852 - recall_6: 0.6435 - val_loss: 0.6598 - val_precision_6: 0.6876 - val_recall_6: 0.6343\n",
            "Epoch 78/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6582 - precision_6: 0.6852 - recall_6: 0.6434 - val_loss: 0.6612 - val_precision_6: 0.6866 - val_recall_6: 0.6340\n",
            "Epoch 79/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6582 - precision_6: 0.6853 - recall_6: 0.6435 - val_loss: 0.6579 - val_precision_6: 0.6847 - val_recall_6: 0.6444\n",
            "Epoch 80/300\n",
            "1280/1280 [==============================] - 12s 9ms/step - loss: 0.6582 - precision_6: 0.6852 - recall_6: 0.6434 - val_loss: 0.6577 - val_precision_6: 0.6849 - val_recall_6: 0.6446\n",
            "Epoch 81/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6582 - precision_6: 0.6853 - recall_6: 0.6435 - val_loss: 0.6575 - val_precision_6: 0.6851 - val_recall_6: 0.6450\n",
            "Epoch 82/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6581 - precision_6: 0.6853 - recall_6: 0.6435 - val_loss: 0.6581 - val_precision_6: 0.6858 - val_recall_6: 0.6428\n",
            "Epoch 83/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6581 - precision_6: 0.6853 - recall_6: 0.6435 - val_loss: 0.6580 - val_precision_6: 0.6850 - val_recall_6: 0.6458\n",
            "Epoch 84/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6581 - precision_6: 0.6853 - recall_6: 0.6435 - val_loss: 0.6578 - val_precision_6: 0.6853 - val_recall_6: 0.6446\n",
            "Epoch 85/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6581 - precision_6: 0.6854 - recall_6: 0.6435 - val_loss: 0.6583 - val_precision_6: 0.6860 - val_recall_6: 0.6397\n",
            "Epoch 86/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6581 - precision_6: 0.6854 - recall_6: 0.6434 - val_loss: 0.6586 - val_precision_6: 0.6833 - val_recall_6: 0.6463\n",
            "Epoch 87/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6581 - precision_6: 0.6853 - recall_6: 0.6434 - val_loss: 0.6581 - val_precision_6: 0.6870 - val_recall_6: 0.6384\n",
            "Epoch 88/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6581 - precision_6: 0.6853 - recall_6: 0.6433 - val_loss: 0.6576 - val_precision_6: 0.6845 - val_recall_6: 0.6481\n",
            "Epoch 89/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6581 - precision_6: 0.6854 - recall_6: 0.6434 - val_loss: 0.6582 - val_precision_6: 0.6850 - val_recall_6: 0.6457\n",
            "Epoch 90/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6581 - precision_6: 0.6854 - recall_6: 0.6434 - val_loss: 0.6600 - val_precision_6: 0.6844 - val_recall_6: 0.6407\n",
            "Epoch 91/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6581 - precision_6: 0.6854 - recall_6: 0.6434 - val_loss: 0.6572 - val_precision_6: 0.6866 - val_recall_6: 0.6420\n",
            "Epoch 92/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6580 - precision_6: 0.6854 - recall_6: 0.6433 - val_loss: 0.6579 - val_precision_6: 0.6865 - val_recall_6: 0.6414\n",
            "Epoch 93/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6580 - precision_6: 0.6855 - recall_6: 0.6433 - val_loss: 0.6597 - val_precision_6: 0.6868 - val_recall_6: 0.6361\n",
            "Epoch 94/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6580 - precision_6: 0.6855 - recall_6: 0.6434 - val_loss: 0.6586 - val_precision_6: 0.6841 - val_recall_6: 0.6428\n",
            "Epoch 95/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6579 - precision_6: 0.6855 - recall_6: 0.6434 - val_loss: 0.6573 - val_precision_6: 0.6861 - val_recall_6: 0.6430\n",
            "Epoch 96/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6580 - precision_6: 0.6854 - recall_6: 0.6432 - val_loss: 0.6574 - val_precision_6: 0.6853 - val_recall_6: 0.6459\n",
            "Epoch 97/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6579 - precision_6: 0.6855 - recall_6: 0.6433 - val_loss: 0.6584 - val_precision_6: 0.6878 - val_recall_6: 0.6361\n",
            "Epoch 98/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6579 - precision_6: 0.6855 - recall_6: 0.6433 - val_loss: 0.6574 - val_precision_6: 0.6866 - val_recall_6: 0.6412\n",
            "Epoch 99/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6579 - precision_6: 0.6855 - recall_6: 0.6433 - val_loss: 0.6581 - val_precision_6: 0.6851 - val_recall_6: 0.6428\n",
            "Epoch 100/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6580 - precision_6: 0.6855 - recall_6: 0.6433 - val_loss: 0.6577 - val_precision_6: 0.6837 - val_recall_6: 0.6471\n",
            "Epoch 101/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6580 - precision_6: 0.6855 - recall_6: 0.6432 - val_loss: 0.6578 - val_precision_6: 0.6860 - val_recall_6: 0.6413\n",
            "Epoch 102/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6579 - precision_6: 0.6855 - recall_6: 0.6433 - val_loss: 0.6572 - val_precision_6: 0.6857 - val_recall_6: 0.6444\n",
            "Epoch 103/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6579 - precision_6: 0.6856 - recall_6: 0.6434 - val_loss: 0.6572 - val_precision_6: 0.6866 - val_recall_6: 0.6420\n",
            "Epoch 104/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6579 - precision_6: 0.6856 - recall_6: 0.6434 - val_loss: 0.6579 - val_precision_6: 0.6843 - val_recall_6: 0.6459\n",
            "Epoch 105/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6579 - precision_6: 0.6856 - recall_6: 0.6434 - val_loss: 0.6574 - val_precision_6: 0.6854 - val_recall_6: 0.6449\n",
            "Epoch 106/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6579 - precision_6: 0.6856 - recall_6: 0.6433 - val_loss: 0.6577 - val_precision_6: 0.6858 - val_recall_6: 0.6422\n",
            "Epoch 107/300\n",
            "1280/1280 [==============================] - 12s 9ms/step - loss: 0.6579 - precision_6: 0.6856 - recall_6: 0.6432 - val_loss: 0.6574 - val_precision_6: 0.6859 - val_recall_6: 0.6426\n",
            "Epoch 108/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6578 - precision_6: 0.6856 - recall_6: 0.6433 - val_loss: 0.6577 - val_precision_6: 0.6859 - val_recall_6: 0.6429\n",
            "Epoch 109/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6579 - precision_6: 0.6856 - recall_6: 0.6433 - val_loss: 0.6585 - val_precision_6: 0.6846 - val_recall_6: 0.6441\n",
            "Epoch 110/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6578 - precision_6: 0.6856 - recall_6: 0.6434 - val_loss: 0.6599 - val_precision_6: 0.6820 - val_recall_6: 0.6461\n",
            "Epoch 111/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6578 - precision_6: 0.6856 - recall_6: 0.6434 - val_loss: 0.6573 - val_precision_6: 0.6878 - val_recall_6: 0.6404\n",
            "Epoch 112/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6578 - precision_6: 0.6856 - recall_6: 0.6434 - val_loss: 0.6594 - val_precision_6: 0.6879 - val_recall_6: 0.6346\n",
            "Epoch 113/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6578 - precision_6: 0.6856 - recall_6: 0.6434 - val_loss: 0.6573 - val_precision_6: 0.6856 - val_recall_6: 0.6443\n",
            "Epoch 114/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6578 - precision_6: 0.6857 - recall_6: 0.6433 - val_loss: 0.6579 - val_precision_6: 0.6837 - val_recall_6: 0.6487\n",
            "Epoch 115/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6578 - precision_6: 0.6856 - recall_6: 0.6435 - val_loss: 0.6583 - val_precision_6: 0.6866 - val_recall_6: 0.6396\n",
            "Epoch 116/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6578 - precision_6: 0.6856 - recall_6: 0.6434 - val_loss: 0.6578 - val_precision_6: 0.6853 - val_recall_6: 0.6448\n",
            "Epoch 117/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6578 - precision_6: 0.6856 - recall_6: 0.6434 - val_loss: 0.6574 - val_precision_6: 0.6864 - val_recall_6: 0.6426\n",
            "Epoch 118/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6578 - precision_6: 0.6856 - recall_6: 0.6434 - val_loss: 0.6574 - val_precision_6: 0.6873 - val_recall_6: 0.6396\n",
            "Epoch 119/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6578 - precision_6: 0.6857 - recall_6: 0.6434 - val_loss: 0.6576 - val_precision_6: 0.6851 - val_recall_6: 0.6444\n",
            "Epoch 120/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6578 - precision_6: 0.6857 - recall_6: 0.6435 - val_loss: 0.6580 - val_precision_6: 0.6836 - val_recall_6: 0.6480\n",
            "Epoch 121/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6578 - precision_6: 0.6856 - recall_6: 0.6434 - val_loss: 0.6576 - val_precision_6: 0.6846 - val_recall_6: 0.6446\n",
            "Epoch 122/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6578 - precision_6: 0.6857 - recall_6: 0.6435 - val_loss: 0.6584 - val_precision_6: 0.6836 - val_recall_6: 0.6471\n",
            "Epoch 123/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6577 - precision_6: 0.6856 - recall_6: 0.6435 - val_loss: 0.6573 - val_precision_6: 0.6861 - val_recall_6: 0.6432\n",
            "Epoch 124/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6578 - precision_6: 0.6856 - recall_6: 0.6436 - val_loss: 0.6587 - val_precision_6: 0.6838 - val_recall_6: 0.6423\n",
            "Epoch 125/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6578 - precision_6: 0.6856 - recall_6: 0.6435 - val_loss: 0.6580 - val_precision_6: 0.6853 - val_recall_6: 0.6428\n",
            "Epoch 126/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6577 - precision_6: 0.6857 - recall_6: 0.6436 - val_loss: 0.6574 - val_precision_6: 0.6848 - val_recall_6: 0.6460\n",
            "Epoch 127/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6577 - precision_6: 0.6857 - recall_6: 0.6436 - val_loss: 0.6583 - val_precision_6: 0.6851 - val_recall_6: 0.6426\n",
            "Epoch 128/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6577 - precision_6: 0.6857 - recall_6: 0.6435 - val_loss: 0.6571 - val_precision_6: 0.6859 - val_recall_6: 0.6440\n",
            "Epoch 129/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6577 - precision_6: 0.6857 - recall_6: 0.6435 - val_loss: 0.6574 - val_precision_6: 0.6858 - val_recall_6: 0.6432\n",
            "Epoch 130/300\n",
            "1280/1280 [==============================] - 12s 9ms/step - loss: 0.6577 - precision_6: 0.6857 - recall_6: 0.6435 - val_loss: 0.6573 - val_precision_6: 0.6855 - val_recall_6: 0.6440\n",
            "Epoch 131/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6577 - precision_6: 0.6857 - recall_6: 0.6435 - val_loss: 0.6571 - val_precision_6: 0.6876 - val_recall_6: 0.6397\n",
            "Epoch 132/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6576 - precision_6: 0.6857 - recall_6: 0.6435 - val_loss: 0.6575 - val_precision_6: 0.6873 - val_recall_6: 0.6393\n",
            "Epoch 133/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6576 - precision_6: 0.6857 - recall_6: 0.6435 - val_loss: 0.6572 - val_precision_6: 0.6849 - val_recall_6: 0.6471\n",
            "Epoch 134/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6576 - precision_6: 0.6857 - recall_6: 0.6436 - val_loss: 0.6572 - val_precision_6: 0.6860 - val_recall_6: 0.6430\n",
            "Epoch 135/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6577 - precision_6: 0.6857 - recall_6: 0.6434 - val_loss: 0.6575 - val_precision_6: 0.6885 - val_recall_6: 0.6365\n",
            "Epoch 136/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6577 - precision_6: 0.6857 - recall_6: 0.6434 - val_loss: 0.6571 - val_precision_6: 0.6866 - val_recall_6: 0.6415\n",
            "Epoch 137/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6577 - precision_6: 0.6857 - recall_6: 0.6434 - val_loss: 0.6571 - val_precision_6: 0.6859 - val_recall_6: 0.6438\n",
            "Epoch 138/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6576 - precision_6: 0.6857 - recall_6: 0.6435 - val_loss: 0.6574 - val_precision_6: 0.6852 - val_recall_6: 0.6443\n",
            "Epoch 139/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6576 - precision_6: 0.6857 - recall_6: 0.6434 - val_loss: 0.6598 - val_precision_6: 0.6874 - val_recall_6: 0.6346\n",
            "Epoch 140/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6576 - precision_6: 0.6858 - recall_6: 0.6435 - val_loss: 0.6570 - val_precision_6: 0.6875 - val_recall_6: 0.6405\n",
            "Epoch 141/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6576 - precision_6: 0.6857 - recall_6: 0.6435 - val_loss: 0.6575 - val_precision_6: 0.6867 - val_recall_6: 0.6409\n",
            "Epoch 142/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6576 - precision_6: 0.6858 - recall_6: 0.6435 - val_loss: 0.6573 - val_precision_6: 0.6874 - val_recall_6: 0.6408\n",
            "Epoch 143/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6576 - precision_6: 0.6857 - recall_6: 0.6435 - val_loss: 0.6572 - val_precision_6: 0.6854 - val_recall_6: 0.6444\n",
            "Epoch 144/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6576 - precision_6: 0.6858 - recall_6: 0.6435 - val_loss: 0.6576 - val_precision_6: 0.6880 - val_recall_6: 0.6379\n",
            "Epoch 145/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6575 - precision_6: 0.6858 - recall_6: 0.6435 - val_loss: 0.6569 - val_precision_6: 0.6865 - val_recall_6: 0.6433\n",
            "Epoch 146/300\n",
            "1280/1280 [==============================] - 12s 9ms/step - loss: 0.6575 - precision_6: 0.6858 - recall_6: 0.6435 - val_loss: 0.6569 - val_precision_6: 0.6861 - val_recall_6: 0.6439\n",
            "Epoch 147/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6575 - precision_6: 0.6859 - recall_6: 0.6435 - val_loss: 0.6571 - val_precision_6: 0.6874 - val_recall_6: 0.6402\n",
            "Epoch 148/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6576 - precision_6: 0.6858 - recall_6: 0.6434 - val_loss: 0.6586 - val_precision_6: 0.6846 - val_recall_6: 0.6424\n",
            "Epoch 149/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6576 - precision_6: 0.6858 - recall_6: 0.6434 - val_loss: 0.6572 - val_precision_6: 0.6852 - val_recall_6: 0.6458\n",
            "Epoch 150/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6576 - precision_6: 0.6858 - recall_6: 0.6434 - val_loss: 0.6600 - val_precision_6: 0.6852 - val_recall_6: 0.6361\n",
            "Epoch 151/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6575 - precision_6: 0.6858 - recall_6: 0.6434 - val_loss: 0.6574 - val_precision_6: 0.6846 - val_recall_6: 0.6479\n",
            "Epoch 152/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6576 - precision_6: 0.6859 - recall_6: 0.6435 - val_loss: 0.6571 - val_precision_6: 0.6869 - val_recall_6: 0.6421\n",
            "Epoch 153/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6575 - precision_6: 0.6858 - recall_6: 0.6434 - val_loss: 0.6572 - val_precision_6: 0.6847 - val_recall_6: 0.6486\n",
            "Epoch 154/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6575 - precision_6: 0.6858 - recall_6: 0.6435 - val_loss: 0.6575 - val_precision_6: 0.6875 - val_recall_6: 0.6398\n",
            "Epoch 155/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6575 - precision_6: 0.6858 - recall_6: 0.6434 - val_loss: 0.6576 - val_precision_6: 0.6866 - val_recall_6: 0.6409\n",
            "Epoch 156/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6575 - precision_6: 0.6859 - recall_6: 0.6435 - val_loss: 0.6577 - val_precision_6: 0.6875 - val_recall_6: 0.6382\n",
            "Epoch 157/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6575 - precision_6: 0.6858 - recall_6: 0.6434 - val_loss: 0.6570 - val_precision_6: 0.6847 - val_recall_6: 0.6471\n",
            "Epoch 158/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6574 - precision_6: 0.6859 - recall_6: 0.6434 - val_loss: 0.6574 - val_precision_6: 0.6870 - val_recall_6: 0.6401\n",
            "Epoch 159/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6575 - precision_6: 0.6859 - recall_6: 0.6434 - val_loss: 0.6573 - val_precision_6: 0.6862 - val_recall_6: 0.6424\n",
            "Epoch 160/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6574 - precision_6: 0.6859 - recall_6: 0.6434 - val_loss: 0.6572 - val_precision_6: 0.6869 - val_recall_6: 0.6419\n",
            "Epoch 161/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6575 - precision_6: 0.6858 - recall_6: 0.6433 - val_loss: 0.6567 - val_precision_6: 0.6853 - val_recall_6: 0.6466\n",
            "Epoch 162/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6575 - precision_6: 0.6859 - recall_6: 0.6434 - val_loss: 0.6571 - val_precision_6: 0.6857 - val_recall_6: 0.6451\n",
            "Epoch 163/300\n",
            "1280/1280 [==============================] - 12s 9ms/step - loss: 0.6575 - precision_6: 0.6859 - recall_6: 0.6433 - val_loss: 0.6580 - val_precision_6: 0.6849 - val_recall_6: 0.6422\n",
            "Epoch 164/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6575 - precision_6: 0.6858 - recall_6: 0.6433 - val_loss: 0.6570 - val_precision_6: 0.6874 - val_recall_6: 0.6402\n",
            "Epoch 165/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6574 - precision_6: 0.6860 - recall_6: 0.6434 - val_loss: 0.6573 - val_precision_6: 0.6862 - val_recall_6: 0.6418\n",
            "Epoch 166/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6574 - precision_6: 0.6859 - recall_6: 0.6433 - val_loss: 0.6573 - val_precision_6: 0.6845 - val_recall_6: 0.6476\n",
            "Epoch 167/300\n",
            "1280/1280 [==============================] - 12s 9ms/step - loss: 0.6575 - precision_6: 0.6859 - recall_6: 0.6433 - val_loss: 0.6583 - val_precision_6: 0.6867 - val_recall_6: 0.6391\n",
            "Epoch 168/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6574 - precision_6: 0.6859 - recall_6: 0.6433 - val_loss: 0.6572 - val_precision_6: 0.6848 - val_recall_6: 0.6467\n",
            "Epoch 169/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6574 - precision_6: 0.6859 - recall_6: 0.6434 - val_loss: 0.6587 - val_precision_6: 0.6838 - val_recall_6: 0.6460\n",
            "Epoch 170/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6574 - precision_6: 0.6859 - recall_6: 0.6434 - val_loss: 0.6571 - val_precision_6: 0.6869 - val_recall_6: 0.6410\n",
            "Epoch 171/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6574 - precision_6: 0.6860 - recall_6: 0.6433 - val_loss: 0.6569 - val_precision_6: 0.6872 - val_recall_6: 0.6412\n",
            "Epoch 172/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6574 - precision_6: 0.6859 - recall_6: 0.6433 - val_loss: 0.6573 - val_precision_6: 0.6851 - val_recall_6: 0.6469\n",
            "Epoch 173/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6574 - precision_6: 0.6860 - recall_6: 0.6432 - val_loss: 0.6567 - val_precision_6: 0.6869 - val_recall_6: 0.6423\n",
            "Epoch 174/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6573 - precision_6: 0.6861 - recall_6: 0.6433 - val_loss: 0.6602 - val_precision_6: 0.6882 - val_recall_6: 0.6307\n",
            "Epoch 175/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6573 - precision_6: 0.6861 - recall_6: 0.6432 - val_loss: 0.6565 - val_precision_6: 0.6865 - val_recall_6: 0.6442\n",
            "Epoch 176/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6573 - precision_6: 0.6861 - recall_6: 0.6431 - val_loss: 0.6567 - val_precision_6: 0.6872 - val_recall_6: 0.6417\n",
            "Epoch 177/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6573 - precision_6: 0.6861 - recall_6: 0.6431 - val_loss: 0.6582 - val_precision_6: 0.6888 - val_recall_6: 0.6353\n",
            "Epoch 178/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6573 - precision_6: 0.6861 - recall_6: 0.6431 - val_loss: 0.6569 - val_precision_6: 0.6862 - val_recall_6: 0.6432\n",
            "Epoch 179/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6573 - precision_6: 0.6860 - recall_6: 0.6432 - val_loss: 0.6569 - val_precision_6: 0.6870 - val_recall_6: 0.6417\n",
            "Epoch 180/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6573 - precision_6: 0.6861 - recall_6: 0.6432 - val_loss: 0.6591 - val_precision_6: 0.6831 - val_recall_6: 0.6485\n",
            "Epoch 181/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6573 - precision_6: 0.6861 - recall_6: 0.6431 - val_loss: 0.6569 - val_precision_6: 0.6863 - val_recall_6: 0.6434\n",
            "Epoch 182/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6573 - precision_6: 0.6861 - recall_6: 0.6432 - val_loss: 0.6574 - val_precision_6: 0.6848 - val_recall_6: 0.6476\n",
            "Epoch 183/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6573 - precision_6: 0.6861 - recall_6: 0.6432 - val_loss: 0.6567 - val_precision_6: 0.6861 - val_recall_6: 0.6457\n",
            "Epoch 184/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6573 - precision_6: 0.6860 - recall_6: 0.6432 - val_loss: 0.6572 - val_precision_6: 0.6852 - val_recall_6: 0.6442\n",
            "Epoch 185/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6573 - precision_6: 0.6862 - recall_6: 0.6433 - val_loss: 0.6594 - val_precision_6: 0.6817 - val_recall_6: 0.6462\n",
            "Epoch 186/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6573 - precision_6: 0.6861 - recall_6: 0.6433 - val_loss: 0.6576 - val_precision_6: 0.6857 - val_recall_6: 0.6433\n",
            "Epoch 187/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6573 - precision_6: 0.6860 - recall_6: 0.6432 - val_loss: 0.6568 - val_precision_6: 0.6852 - val_recall_6: 0.6471\n",
            "Epoch 188/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6573 - precision_6: 0.6861 - recall_6: 0.6433 - val_loss: 0.6579 - val_precision_6: 0.6867 - val_recall_6: 0.6385\n",
            "Epoch 189/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6572 - precision_6: 0.6861 - recall_6: 0.6433 - val_loss: 0.6580 - val_precision_6: 0.6843 - val_recall_6: 0.6455\n",
            "Epoch 190/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6572 - precision_6: 0.6862 - recall_6: 0.6433 - val_loss: 0.6570 - val_precision_6: 0.6857 - val_recall_6: 0.6454\n",
            "Epoch 191/300\n",
            "1280/1280 [==============================] - 12s 9ms/step - loss: 0.6572 - precision_6: 0.6861 - recall_6: 0.6433 - val_loss: 0.6568 - val_precision_6: 0.6876 - val_recall_6: 0.6408\n",
            "Epoch 192/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6573 - precision_6: 0.6862 - recall_6: 0.6433 - val_loss: 0.6574 - val_precision_6: 0.6856 - val_recall_6: 0.6425\n",
            "Epoch 193/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6572 - precision_6: 0.6862 - recall_6: 0.6433 - val_loss: 0.6575 - val_precision_6: 0.6856 - val_recall_6: 0.6434\n",
            "Epoch 194/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6572 - precision_6: 0.6861 - recall_6: 0.6432 - val_loss: 0.6584 - val_precision_6: 0.6846 - val_recall_6: 0.6436\n",
            "Epoch 195/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6572 - precision_6: 0.6862 - recall_6: 0.6433 - val_loss: 0.6575 - val_precision_6: 0.6849 - val_recall_6: 0.6480\n",
            "Epoch 196/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6572 - precision_6: 0.6862 - recall_6: 0.6433 - val_loss: 0.6574 - val_precision_6: 0.6842 - val_recall_6: 0.6450\n",
            "Epoch 197/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6572 - precision_6: 0.6861 - recall_6: 0.6434 - val_loss: 0.6574 - val_precision_6: 0.6868 - val_recall_6: 0.6399\n",
            "Epoch 198/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6572 - precision_6: 0.6861 - recall_6: 0.6434 - val_loss: 0.6575 - val_precision_6: 0.6872 - val_recall_6: 0.6386\n",
            "Epoch 199/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6572 - precision_6: 0.6861 - recall_6: 0.6432 - val_loss: 0.6580 - val_precision_6: 0.6842 - val_recall_6: 0.6468\n",
            "Epoch 200/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6573 - precision_6: 0.6861 - recall_6: 0.6433 - val_loss: 0.6569 - val_precision_6: 0.6878 - val_recall_6: 0.6396\n",
            "Epoch 201/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6572 - precision_6: 0.6862 - recall_6: 0.6433 - val_loss: 0.6579 - val_precision_6: 0.6863 - val_recall_6: 0.6411\n",
            "Epoch 202/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6572 - precision_6: 0.6862 - recall_6: 0.6435 - val_loss: 0.6573 - val_precision_6: 0.6877 - val_recall_6: 0.6387\n",
            "Epoch 203/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6572 - precision_6: 0.6862 - recall_6: 0.6433 - val_loss: 0.6567 - val_precision_6: 0.6879 - val_recall_6: 0.6398\n",
            "Epoch 204/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6572 - precision_6: 0.6862 - recall_6: 0.6434 - val_loss: 0.6571 - val_precision_6: 0.6842 - val_recall_6: 0.6481\n",
            "Epoch 205/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6571 - precision_6: 0.6863 - recall_6: 0.6434 - val_loss: 0.6568 - val_precision_6: 0.6870 - val_recall_6: 0.6412\n",
            "Epoch 206/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6572 - precision_6: 0.6862 - recall_6: 0.6434 - val_loss: 0.6575 - val_precision_6: 0.6870 - val_recall_6: 0.6386\n",
            "Epoch 207/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6572 - precision_6: 0.6862 - recall_6: 0.6434 - val_loss: 0.6570 - val_precision_6: 0.6862 - val_recall_6: 0.6434\n",
            "Epoch 208/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6571 - precision_6: 0.6862 - recall_6: 0.6434 - val_loss: 0.6575 - val_precision_6: 0.6872 - val_recall_6: 0.6388\n",
            "Epoch 209/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6572 - precision_6: 0.6862 - recall_6: 0.6434 - val_loss: 0.6578 - val_precision_6: 0.6884 - val_recall_6: 0.6357\n",
            "Epoch 210/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6571 - precision_6: 0.6862 - recall_6: 0.6434 - val_loss: 0.6574 - val_precision_6: 0.6843 - val_recall_6: 0.6484\n",
            "Epoch 211/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6571 - precision_6: 0.6862 - recall_6: 0.6434 - val_loss: 0.6563 - val_precision_6: 0.6876 - val_recall_6: 0.6426\n",
            "Epoch 212/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6571 - precision_6: 0.6862 - recall_6: 0.6435 - val_loss: 0.6565 - val_precision_6: 0.6854 - val_recall_6: 0.6457\n",
            "Epoch 213/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6571 - precision_6: 0.6862 - recall_6: 0.6434 - val_loss: 0.6566 - val_precision_6: 0.6879 - val_recall_6: 0.6399\n",
            "Epoch 214/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6571 - precision_6: 0.6862 - recall_6: 0.6434 - val_loss: 0.6577 - val_precision_6: 0.6884 - val_recall_6: 0.6355\n",
            "Epoch 215/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6571 - precision_6: 0.6863 - recall_6: 0.6435 - val_loss: 0.6567 - val_precision_6: 0.6880 - val_recall_6: 0.6400\n",
            "Epoch 216/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6571 - precision_6: 0.6863 - recall_6: 0.6434 - val_loss: 0.6584 - val_precision_6: 0.6868 - val_recall_6: 0.6361\n",
            "Epoch 217/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6571 - precision_6: 0.6862 - recall_6: 0.6435 - val_loss: 0.6575 - val_precision_6: 0.6884 - val_recall_6: 0.6369\n",
            "Epoch 218/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6571 - precision_6: 0.6863 - recall_6: 0.6434 - val_loss: 0.6565 - val_precision_6: 0.6865 - val_recall_6: 0.6440\n",
            "Epoch 219/300\n",
            "1280/1280 [==============================] - 12s 9ms/step - loss: 0.6571 - precision_6: 0.6862 - recall_6: 0.6435 - val_loss: 0.6566 - val_precision_6: 0.6863 - val_recall_6: 0.6449\n",
            "Epoch 220/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6571 - precision_6: 0.6863 - recall_6: 0.6435 - val_loss: 0.6565 - val_precision_6: 0.6873 - val_recall_6: 0.6419\n",
            "Epoch 221/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6570 - precision_6: 0.6863 - recall_6: 0.6435 - val_loss: 0.6567 - val_precision_6: 0.6870 - val_recall_6: 0.6417\n",
            "Epoch 222/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6571 - precision_6: 0.6863 - recall_6: 0.6435 - val_loss: 0.6564 - val_precision_6: 0.6862 - val_recall_6: 0.6444\n",
            "Epoch 223/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6571 - precision_6: 0.6863 - recall_6: 0.6434 - val_loss: 0.6567 - val_precision_6: 0.6850 - val_recall_6: 0.6458\n",
            "Epoch 224/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6570 - precision_6: 0.6863 - recall_6: 0.6435 - val_loss: 0.6569 - val_precision_6: 0.6867 - val_recall_6: 0.6429\n",
            "Epoch 225/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6571 - precision_6: 0.6863 - recall_6: 0.6435 - val_loss: 0.6577 - val_precision_6: 0.6871 - val_recall_6: 0.6382\n",
            "Epoch 226/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6570 - precision_6: 0.6863 - recall_6: 0.6434 - val_loss: 0.6566 - val_precision_6: 0.6858 - val_recall_6: 0.6457\n",
            "Epoch 227/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6570 - precision_6: 0.6864 - recall_6: 0.6436 - val_loss: 0.6571 - val_precision_6: 0.6865 - val_recall_6: 0.6429\n",
            "Epoch 228/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6570 - precision_6: 0.6864 - recall_6: 0.6436 - val_loss: 0.6565 - val_precision_6: 0.6867 - val_recall_6: 0.6442\n",
            "Epoch 229/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6570 - precision_6: 0.6864 - recall_6: 0.6436 - val_loss: 0.6575 - val_precision_6: 0.6850 - val_recall_6: 0.6464\n",
            "Epoch 230/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6570 - precision_6: 0.6864 - recall_6: 0.6435 - val_loss: 0.6571 - val_precision_6: 0.6873 - val_recall_6: 0.6401\n",
            "Epoch 231/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6570 - precision_6: 0.6864 - recall_6: 0.6435 - val_loss: 0.6564 - val_precision_6: 0.6872 - val_recall_6: 0.6436\n",
            "Epoch 232/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6570 - precision_6: 0.6864 - recall_6: 0.6434 - val_loss: 0.6575 - val_precision_6: 0.6871 - val_recall_6: 0.6393\n",
            "Epoch 233/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6570 - precision_6: 0.6864 - recall_6: 0.6436 - val_loss: 0.6575 - val_precision_6: 0.6880 - val_recall_6: 0.6387\n",
            "Epoch 234/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6571 - precision_6: 0.6864 - recall_6: 0.6435 - val_loss: 0.6568 - val_precision_6: 0.6860 - val_recall_6: 0.6437\n",
            "Epoch 235/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6570 - precision_6: 0.6864 - recall_6: 0.6436 - val_loss: 0.6564 - val_precision_6: 0.6856 - val_recall_6: 0.6459\n",
            "Epoch 236/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6570 - precision_6: 0.6864 - recall_6: 0.6435 - val_loss: 0.6564 - val_precision_6: 0.6868 - val_recall_6: 0.6439\n",
            "Epoch 237/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6570 - precision_6: 0.6864 - recall_6: 0.6435 - val_loss: 0.6565 - val_precision_6: 0.6866 - val_recall_6: 0.6445\n",
            "Epoch 238/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6570 - precision_6: 0.6864 - recall_6: 0.6436 - val_loss: 0.6568 - val_precision_6: 0.6869 - val_recall_6: 0.6433\n",
            "Epoch 239/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6570 - precision_6: 0.6864 - recall_6: 0.6435 - val_loss: 0.6570 - val_precision_6: 0.6856 - val_recall_6: 0.6462\n",
            "Epoch 240/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6570 - precision_6: 0.6864 - recall_6: 0.6436 - val_loss: 0.6565 - val_precision_6: 0.6875 - val_recall_6: 0.6410\n",
            "Epoch 241/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6569 - precision_6: 0.6865 - recall_6: 0.6436 - val_loss: 0.6583 - val_precision_6: 0.6834 - val_recall_6: 0.6508\n",
            "Epoch 242/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6570 - precision_6: 0.6865 - recall_6: 0.6436 - val_loss: 0.6562 - val_precision_6: 0.6878 - val_recall_6: 0.6419\n",
            "Epoch 243/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6570 - precision_6: 0.6865 - recall_6: 0.6435 - val_loss: 0.6564 - val_precision_6: 0.6876 - val_recall_6: 0.6423\n",
            "Epoch 244/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6569 - precision_6: 0.6865 - recall_6: 0.6436 - val_loss: 0.6569 - val_precision_6: 0.6873 - val_recall_6: 0.6406\n",
            "Epoch 245/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6569 - precision_6: 0.6865 - recall_6: 0.6435 - val_loss: 0.6564 - val_precision_6: 0.6864 - val_recall_6: 0.6442\n",
            "Epoch 246/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6569 - precision_6: 0.6865 - recall_6: 0.6437 - val_loss: 0.6567 - val_precision_6: 0.6887 - val_recall_6: 0.6384\n",
            "Epoch 247/300\n",
            "1280/1280 [==============================] - 12s 9ms/step - loss: 0.6569 - precision_6: 0.6864 - recall_6: 0.6435 - val_loss: 0.6584 - val_precision_6: 0.6885 - val_recall_6: 0.6342\n",
            "Epoch 248/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6570 - precision_6: 0.6865 - recall_6: 0.6434 - val_loss: 0.6571 - val_precision_6: 0.6845 - val_recall_6: 0.6497\n",
            "Epoch 249/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6570 - precision_6: 0.6865 - recall_6: 0.6435 - val_loss: 0.6586 - val_precision_6: 0.6831 - val_recall_6: 0.6418\n",
            "Epoch 250/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6570 - precision_6: 0.6865 - recall_6: 0.6436 - val_loss: 0.6567 - val_precision_6: 0.6873 - val_recall_6: 0.6410\n",
            "Epoch 251/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6570 - precision_6: 0.6864 - recall_6: 0.6435 - val_loss: 0.6567 - val_precision_6: 0.6850 - val_recall_6: 0.6494\n",
            "Epoch 252/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6570 - precision_6: 0.6865 - recall_6: 0.6436 - val_loss: 0.6568 - val_precision_6: 0.6881 - val_recall_6: 0.6395\n",
            "Epoch 253/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6569 - precision_6: 0.6865 - recall_6: 0.6435 - val_loss: 0.6577 - val_precision_6: 0.6839 - val_recall_6: 0.6445\n",
            "Epoch 254/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6569 - precision_6: 0.6865 - recall_6: 0.6435 - val_loss: 0.6586 - val_precision_6: 0.6841 - val_recall_6: 0.6477\n",
            "Epoch 255/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6569 - precision_6: 0.6866 - recall_6: 0.6435 - val_loss: 0.6562 - val_precision_6: 0.6866 - val_recall_6: 0.6453\n",
            "Epoch 256/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6569 - precision_6: 0.6865 - recall_6: 0.6435 - val_loss: 0.6566 - val_precision_6: 0.6860 - val_recall_6: 0.6440\n",
            "Epoch 257/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6569 - precision_6: 0.6865 - recall_6: 0.6436 - val_loss: 0.6566 - val_precision_6: 0.6862 - val_recall_6: 0.6463\n",
            "Epoch 258/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6569 - precision_6: 0.6865 - recall_6: 0.6435 - val_loss: 0.6569 - val_precision_6: 0.6858 - val_recall_6: 0.6441\n",
            "Epoch 259/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6569 - precision_6: 0.6865 - recall_6: 0.6434 - val_loss: 0.6579 - val_precision_6: 0.6841 - val_recall_6: 0.6432\n",
            "Epoch 260/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6569 - precision_6: 0.6865 - recall_6: 0.6435 - val_loss: 0.6567 - val_precision_6: 0.6872 - val_recall_6: 0.6405\n",
            "Epoch 261/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6569 - precision_6: 0.6865 - recall_6: 0.6434 - val_loss: 0.6575 - val_precision_6: 0.6879 - val_recall_6: 0.6364\n",
            "Epoch 262/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6570 - precision_6: 0.6865 - recall_6: 0.6433 - val_loss: 0.6578 - val_precision_6: 0.6853 - val_recall_6: 0.6445\n",
            "Epoch 263/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6569 - precision_6: 0.6866 - recall_6: 0.6435 - val_loss: 0.6579 - val_precision_6: 0.6893 - val_recall_6: 0.6344\n",
            "Epoch 264/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6569 - precision_6: 0.6865 - recall_6: 0.6436 - val_loss: 0.6566 - val_precision_6: 0.6878 - val_recall_6: 0.6408\n",
            "Epoch 265/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6569 - precision_6: 0.6865 - recall_6: 0.6434 - val_loss: 0.6567 - val_precision_6: 0.6858 - val_recall_6: 0.6468\n",
            "Epoch 266/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6569 - precision_6: 0.6866 - recall_6: 0.6435 - val_loss: 0.6568 - val_precision_6: 0.6858 - val_recall_6: 0.6452\n",
            "Epoch 267/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6569 - precision_6: 0.6866 - recall_6: 0.6435 - val_loss: 0.6576 - val_precision_6: 0.6843 - val_recall_6: 0.6509\n",
            "Epoch 268/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6569 - precision_6: 0.6866 - recall_6: 0.6435 - val_loss: 0.6566 - val_precision_6: 0.6887 - val_recall_6: 0.6381\n",
            "Epoch 269/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6569 - precision_6: 0.6866 - recall_6: 0.6434 - val_loss: 0.6562 - val_precision_6: 0.6866 - val_recall_6: 0.6461\n",
            "Epoch 270/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6568 - precision_6: 0.6866 - recall_6: 0.6435 - val_loss: 0.6561 - val_precision_6: 0.6880 - val_recall_6: 0.6420\n",
            "Epoch 271/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6569 - precision_6: 0.6866 - recall_6: 0.6434 - val_loss: 0.6567 - val_precision_6: 0.6853 - val_recall_6: 0.6461\n",
            "Epoch 272/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6569 - precision_6: 0.6866 - recall_6: 0.6435 - val_loss: 0.6564 - val_precision_6: 0.6873 - val_recall_6: 0.6436\n",
            "Epoch 273/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6569 - precision_6: 0.6866 - recall_6: 0.6435 - val_loss: 0.6571 - val_precision_6: 0.6871 - val_recall_6: 0.6411\n",
            "Epoch 274/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6569 - precision_6: 0.6866 - recall_6: 0.6435 - val_loss: 0.6563 - val_precision_6: 0.6869 - val_recall_6: 0.6449\n",
            "Epoch 275/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6569 - precision_6: 0.6866 - recall_6: 0.6433 - val_loss: 0.6567 - val_precision_6: 0.6856 - val_recall_6: 0.6446\n",
            "Epoch 276/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6569 - precision_6: 0.6866 - recall_6: 0.6433 - val_loss: 0.6570 - val_precision_6: 0.6860 - val_recall_6: 0.6448\n",
            "Epoch 277/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6569 - precision_6: 0.6866 - recall_6: 0.6434 - val_loss: 0.6570 - val_precision_6: 0.6870 - val_recall_6: 0.6424\n",
            "Epoch 278/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6569 - precision_6: 0.6867 - recall_6: 0.6434 - val_loss: 0.6576 - val_precision_6: 0.6890 - val_recall_6: 0.6362\n",
            "Epoch 279/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6569 - precision_6: 0.6866 - recall_6: 0.6434 - val_loss: 0.6567 - val_precision_6: 0.6869 - val_recall_6: 0.6438\n",
            "Epoch 280/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6568 - precision_6: 0.6867 - recall_6: 0.6434 - val_loss: 0.6567 - val_precision_6: 0.6863 - val_recall_6: 0.6458\n",
            "Epoch 281/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6568 - precision_6: 0.6867 - recall_6: 0.6435 - val_loss: 0.6568 - val_precision_6: 0.6853 - val_recall_6: 0.6439\n",
            "Epoch 282/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6569 - precision_6: 0.6866 - recall_6: 0.6434 - val_loss: 0.6566 - val_precision_6: 0.6888 - val_recall_6: 0.6384\n",
            "Epoch 283/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6568 - precision_6: 0.6866 - recall_6: 0.6434 - val_loss: 0.6570 - val_precision_6: 0.6875 - val_recall_6: 0.6398\n",
            "Epoch 284/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6568 - precision_6: 0.6867 - recall_6: 0.6435 - val_loss: 0.6566 - val_precision_6: 0.6871 - val_recall_6: 0.6434\n",
            "Epoch 285/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6568 - precision_6: 0.6867 - recall_6: 0.6434 - val_loss: 0.6563 - val_precision_6: 0.6865 - val_recall_6: 0.6444\n",
            "Epoch 286/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6568 - precision_6: 0.6867 - recall_6: 0.6433 - val_loss: 0.6576 - val_precision_6: 0.6854 - val_recall_6: 0.6435\n",
            "Epoch 287/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6568 - precision_6: 0.6867 - recall_6: 0.6433 - val_loss: 0.6564 - val_precision_6: 0.6890 - val_recall_6: 0.6389\n",
            "Epoch 288/300\n",
            "1280/1280 [==============================] - 12s 9ms/step - loss: 0.6568 - precision_6: 0.6867 - recall_6: 0.6433 - val_loss: 0.6585 - val_precision_6: 0.6838 - val_recall_6: 0.6447\n",
            "Epoch 289/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6569 - precision_6: 0.6867 - recall_6: 0.6434 - val_loss: 0.6561 - val_precision_6: 0.6878 - val_recall_6: 0.6423\n",
            "Epoch 290/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6569 - precision_6: 0.6867 - recall_6: 0.6433 - val_loss: 0.6568 - val_precision_6: 0.6865 - val_recall_6: 0.6419\n",
            "Epoch 291/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6568 - precision_6: 0.6868 - recall_6: 0.6434 - val_loss: 0.6564 - val_precision_6: 0.6853 - val_recall_6: 0.6479\n",
            "Epoch 292/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6568 - precision_6: 0.6868 - recall_6: 0.6433 - val_loss: 0.6567 - val_precision_6: 0.6886 - val_recall_6: 0.6380\n",
            "Epoch 293/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6568 - precision_6: 0.6867 - recall_6: 0.6433 - val_loss: 0.6561 - val_precision_6: 0.6865 - val_recall_6: 0.6460\n",
            "Epoch 294/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6568 - precision_6: 0.6867 - recall_6: 0.6433 - val_loss: 0.6564 - val_precision_6: 0.6862 - val_recall_6: 0.6461\n",
            "Epoch 295/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6568 - precision_6: 0.6868 - recall_6: 0.6433 - val_loss: 0.6567 - val_precision_6: 0.6854 - val_recall_6: 0.6484\n",
            "Epoch 296/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6568 - precision_6: 0.6868 - recall_6: 0.6433 - val_loss: 0.6571 - val_precision_6: 0.6865 - val_recall_6: 0.6421\n",
            "Epoch 297/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6569 - precision_6: 0.6867 - recall_6: 0.6433 - val_loss: 0.6573 - val_precision_6: 0.6845 - val_recall_6: 0.6496\n",
            "Epoch 298/300\n",
            "1280/1280 [==============================] - 11s 8ms/step - loss: 0.6569 - precision_6: 0.6867 - recall_6: 0.6433 - val_loss: 0.6569 - val_precision_6: 0.6871 - val_recall_6: 0.6405\n",
            "Epoch 299/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6569 - precision_6: 0.6867 - recall_6: 0.6433 - val_loss: 0.6563 - val_precision_6: 0.6875 - val_recall_6: 0.6421\n",
            "Epoch 300/300\n",
            "1280/1280 [==============================] - 11s 9ms/step - loss: 0.6569 - precision_6: 0.6867 - recall_6: 0.6432 - val_loss: 0.6564 - val_precision_6: 0.6855 - val_recall_6: 0.6488\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_optimal_epoch(history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTq4Ki90Xan-",
        "outputId": "ca938e60-57e4-4fce-b01c-b29bb2b64acb"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "293"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model('model-tpu.keras', x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y79LHKKukb0i",
        "outputId": "89f61b80-1c6b-41fa-d0fb-09d7cc96b476"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "160/160 [==============================] - 2s 12ms/step - loss: 0.6568 - precision_6: 0.6866 - recall_6: 0.6455\n",
            "Loss: 0.657, Precision: 0.687, Recall: 0.645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp model-tpu.keras /content/drive/MyDrive/model.keras"
      ],
      "metadata": {
        "id": "F4IVsCFykpti"
      },
      "execution_count": 51,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "rta.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}